+++

date = "2019-02-22T20:20:20+08:00"
title = "支持向量机SVM(一)"
categories = ["技术文章"]
tags = ["machine learning", "SVM"]
author = "月光晒谷"
author_homepage =  "http://liuao.tech"

+++



&nbsp; &nbsp; &nbsp; &nbsp;支持向量机（Support Vector Machines SVM）是一种二分类模型，它的目标是在特征空间中寻找对于所有样本距离最大的超平面。与感知机不同的是，在线性可分的情况下，SVM可以得到唯一的解。

<!--more-->

&nbsp; &nbsp; &nbsp; &nbsp;假设训练集样本为：$T= \\{(x\_{1},y\_{1}),(x\_{2},y\_{2}), ... ,(x\_{N},y\_{N}) \\}$，其中$x\_{i}$ $ \in $ $\mathbb R^{n}$表示样本的属性，$y\_{i}$ $\in$ $ \\{ +1, -1\\}$表示样本的标签，$i=1,2,...,N$。

&nbsp; &nbsp; &nbsp; &nbsp;设SVM所求的超平面为$w \cdot x + b = 0$ 其中$w \in \mathbb R^{n}$，$b \in \mathbb R$，定义某个样本点$(x\_{i}, y\_{i})$到超平面$w \cdot x + b = 0$的距离为：

$$d\_{i} =   \frac{y\_{i}(w \cdot x\_{i} + b)}{\parallel w \parallel}$$


令$d = \min d\_{i} $， $i=1,2,...,N$,则SVM的目标函数为：
$$\begin{cases}
\\ \max \qquad d \\\\\\
\\ s.t. \qquad \frac{y\_{i}(w \cdot x\_{i} + b)}{\parallel w \parallel} \geq d \qquad i=1,2,...,N \end{cases}$$

令$ d = \frac{\hat d} {\parallel w \parallel}$，则上式可化为：
$$\begin{cases}
\\ \max \qquad \frac{\hat d} {\parallel w \parallel} \\\\\\
\\ s.t. \qquad y\_{i}(w \cdot x\_{i} + b) \geq \hat d \qquad i=1,2,...,N \end{cases}$$


&nbsp; &nbsp; &nbsp; &nbsp;由于超平面$w \cdot x + b = 0$可以对其参数进行任意大小的缩放，即对于任意的标量$\lambda$，都有$\lambda(w \cdot x)$ $ + \lambda b = 0$，$\hat d$为一个标量，它的大小对于最终的参数求解并无影响，因此可以令$\hat d = 1$。并且$\max \frac{1}{\parallel w \parallel}$等价于$\min \frac{1}{2}\parallel w \parallel^{2}$。因此上式等价为：

$$\begin{cases}
\\ \min \qquad \frac{1} {2}\parallel w\parallel^{2} \\\\\\
\\ s.t. \qquad y\_{i}(w \cdot x\_{i} + b) \geq 1 \qquad i=1,2,...,N \end{cases} \qquad (1)$$ 


&nbsp; &nbsp; &nbsp; &nbsp;接下来可以通过拉格朗日对偶性，求解该问题的对偶问题，从而得到原始问题的最优解。引入拉格朗日因子$\lambda \_ {i} \geq 0, i=1,2,...,N$，则拉格朗日函数为：

$$L(w,b,\lambda) = \frac{1} {2}\parallel w\parallel^{2} + \sum\_{i=1}^{N} \lambda \_ {i}(1-y\_{i}(w \cdot x\_{i} + b)) \qquad (2)$$

由于$\lambda \_ {i} ( 1-y\_{i}(w \cdot x\_{i} + b)) \leq 0, i=1,2,...,N$，因此
$$\frac{1} {2}\parallel w \parallel^{2} \geq L(w,b,\lambda)，等号成立的条件为：\sum\_{i=1}^{N} \lambda \_ {i}(1-y\_{i}(w \cdot x\_{i} + b)) = 0 \qquad (3)$$

所以(1)可以等价为：

$$\min\limits\_{w,b} \max\limits\_{\lambda} L(w,b,\lambda)  \qquad (4) $$

根据拉格朗日对偶性，上述等式的对偶问题是极大极小问题：

$$\max\limits\_{\lambda} \min\limits\_{w,b}  L(w,b,\lambda) \qquad (5)$$ 


对于上式，先求解$\min\limits\_{w,b}  L(w,b,\lambda)$.首先对其关于$w$与$b$分别求偏导，并令其为0：

$$\frac{\partial L}{\partial w} = w - \sum\_{i=1}^{N} \lambda\_{i}y\_{i}x\_{i}=0 \qquad \frac{\partial L}{\partial b} = -  \sum\_{i=1}^{N}\lambda\_{i}y\_{i}=0$$

则：

$$w = \sum\_{i=1}^{N} \lambda\_{i}y\_{i}x\_{i} \qquad \sum\_{i=1}^{N}\lambda\_{i}y\_{i}=0 \qquad (6)$$

将上述结果带入到拉格朗日函数可得：

$$L(w,b,\lambda) = - \frac{1}{2} \sum\_{i=1}^{N}\sum\_{j=1}^{N}\lambda\_{i}\lambda\_{j}y\_{i}y\_{j}(x\_{i} \cdot x\_{j}) + \sum\_{i=1}^{N} \lambda \_{i} \qquad (7)$$

则(5)可以化为：

$$\begin{cases}
\\ \max \qquad - \frac{1}{2} \sum\_{i=1}^{N}\sum\_{j=1}^{N}\lambda\_{i}\lambda\_{j}y\_{i}y\_{j}(x\_{i} \cdot x\_{j}) + \sum\_{i=1}^{N} \lambda \_{i} \\\\\\\\\\\\
\\ s.t.\qquad \sum\_{i=1}^{N}\lambda\_{i}y\_{i}=0 ,\qquad\lambda\_{i} \geq 0 \qquad i=1,2,...,N \end{cases} \qquad (8)$$ 


&nbsp; &nbsp; &nbsp; &nbsp;通过上式可以求出原始问题的对偶优化问题，求解该问题可以借用序列最小化算法（Sequential minimal optimization SMO）。使用对偶问题求解带来的一大好处是：在求解中只使用了原始样本属性的内积形式$x\_{i} \cdot x\_{j}$，这为之后引入核函数提供了很大的便利。

&nbsp; &nbsp; &nbsp; &nbsp;另外考虑(3)中的等号成立的条件，由此可知：

$$\begin{cases}
\\ 当 y\_{i}(w \cdot x\_{i} + b) > 1 时，\lambda\_{i} = 0\\\\\\
\\ 当 y\_{i}(w \cdot x\_{i} + b) = 1 时，\lambda\_{i} \geq 0 \end{cases} \qquad (9)$$ 

因此只有样本$i$满足：$y\_{i}(w \cdot x\_{i} + b) = 1$时，该样本对于最终的结果才会有影响。这些样本处于边界位置，被称为__支持向量__。

&nbsp; &nbsp; &nbsp; &nbsp;通过(8)可以解出$\lambda\_{i}(i=1,2,...,N)$的值，然后将其带入(6)中，可以解出参数$w$的解。另外当$\lambda\_{j} > 0$时，对应的样本点为支持向量，该点满足条件：$y\_{j}(w \cdot x\_{j} + b) = 1$。将参数$w$的解带入该条件中，可以得出参数$b$的解。因此最终的解形式为：


$$\begin{cases}
\\ w = \sum\_{i=1}^{N} \lambda\_{i}y\_{i}x\_{i}\\\\\\\\\\
\\ b = y\_{j} - \sum\_{i=1}^{N} \lambda\_{i}y\_{i}(x\_{i} \cdot x\_{j}) \qquad \lambda\_{j} > 0 \end{cases} \qquad (10)$$ 
***

1.参考文档：

&nbsp; &nbsp; &nbsp; &nbsp;[1]. 统计学习方法&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;李航 著